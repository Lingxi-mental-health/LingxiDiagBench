#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Hugging Face Dataset ä¸‹è½½å·¥å…·

ç”¨äºä» Hugging Face Hub ä¸‹è½½ LingxiDiag-16K æ•°æ®é›†ã€‚
æ”¯æŒä½¿ç”¨ hf-mirror é•œåƒåŠ é€Ÿä¸‹è½½ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    # ä¸‹è½½æ•°æ®é›†ï¼ˆä½¿ç”¨é•œåƒï¼‰
    python scripts/huggingface_download.py \
        --repo-name "your_username/lingxidiag-16k" \
        --output-dir "./downloaded_data" \
        --token "your_huggingface_token"
    
    # ä¸‹è½½æ•°æ®é›†ï¼ˆä¸ä½¿ç”¨é•œåƒï¼‰
    python scripts/huggingface_download.py \
        --repo-name "your_username/lingxidiag-16k" \
        --output-dir "./downloaded_data" \
        --no-mirror

ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼‰:
    export HF_TOKEN=your_huggingface_token
"""

import json
import os
import sys
import argparse
import logging
import shutil
import subprocess
from pathlib import Path
from typing import Dict, List, Optional, Union

import pandas as pd
from datasets import load_dataset
from huggingface_hub import HfApi, login

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# é¡¹ç›®è·¯å¾„é…ç½®
PROJECT_ROOT = Path(__file__).parent.parent


class LingxiDatasetDownloader:
    """
    LingxiDiag-16K æ•°æ®é›†ä¸‹è½½å·¥å…·ç±»
    æ”¯æŒä½¿ç”¨hf-mirroré•œåƒåŠ é€Ÿä¸‹è½½
    """
    
    def __init__(self, use_mirror: bool = True, mirror_url: str = "https://hf-mirror.com"):
        self.api = HfApi()
        self.use_mirror = use_mirror
        self.mirror_url = mirror_url
        
        if self.use_mirror:
            self._setup_mirror()
    
    def _setup_mirror(self):
        """è®¾ç½®Hugging Faceé•œåƒ"""
        logger.info(f"ğŸª é…ç½®Hugging Faceé•œåƒ: {self.mirror_url}")
        os.environ["HF_ENDPOINT"] = self.mirror_url
        
        try:
            import datasets
            datasets.config.HF_ENDPOINT = self.mirror_url
            logger.info("âœ… é•œåƒé…ç½®å®Œæˆ")
        except Exception as e:
            logger.warning(f"âš ï¸ é•œåƒé…ç½®è­¦å‘Š: {str(e)}")
    
    def validate_token(self, token: str) -> bool:
        """
        éªŒè¯ Hugging Face token æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            token: Hugging Face è®¿é—®ä»¤ç‰Œ
            
        Returns:
            token æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            api = HfApi(token=token)
            user_info = api.whoami()
            logger.info(f"âœ… Token éªŒè¯æˆåŠŸï¼Œç”¨æˆ·: {user_info['name']}")
            return True
        except Exception as e:
            logger.error(f"âŒ Token éªŒè¯å¤±è´¥: {str(e)}")
            return False
    
    def check_dataset_access(self, repo_name: str, token: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰è®¿é—®æ•°æ®é›†çš„æƒé™
        
        Args:
            repo_name: æ•°æ®é›†åç§°
            token: è®¿é—®ä»¤ç‰Œ
            
        Returns:
            æ˜¯å¦æœ‰è®¿é—®æƒé™
        """
        try:
            api = HfApi(token=token)
            dataset_info = api.dataset_info(repo_name)
            logger.info(f"âœ… æ•°æ®é›†è®¿é—®æƒé™éªŒè¯æˆåŠŸ: {repo_name}")
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®é›†è®¿é—®æƒé™éªŒè¯å¤±è´¥: {str(e)}")
            if "401" in str(e):
                logger.info("ğŸ’¡ å¯èƒ½çš„åŸå› :")
                logger.info("   1. Token æ²¡æœ‰è®¿é—®è¯¥æ•°æ®é›†çš„æƒé™")
                logger.info("   2. æ•°æ®é›†æ˜¯ç§æœ‰çš„ï¼Œä½†æ‚¨ä¸æ˜¯åä½œè€…")
                logger.info("   3. Token ä½œç”¨åŸŸæƒé™ä¸è¶³")
            return False
    
    def download_dataset_cli(
        self, 
        repo_name: str, 
        token: Optional[str] = None, 
        local_dir: str = "./temp_dataset"
    ) -> str:
        """
        ä½¿ç”¨ huggingface-cli ä¸‹è½½æ•°æ®é›†ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼‰
        
        Args:
            repo_name: ä»“åº“åç§°
            token: è®¿é—®ä»¤ç‰Œ
            local_dir: æœ¬åœ°ä¸‹è½½ç›®å½•
            
        Returns:
            ä¸‹è½½çš„æœ¬åœ°ç›®å½•è·¯å¾„
        """
        logger.info(f"ğŸ”§ ä½¿ç”¨ CLI å·¥å…·ä¸‹è½½æ•°æ®é›†: {repo_name}")
        
        local_path = Path(local_dir)
        local_path.mkdir(parents=True, exist_ok=True)
        
        env = os.environ.copy()
        if self.use_mirror:
            env["HF_ENDPOINT"] = self.mirror_url
            logger.info(f"ğŸª è®¾ç½®é•œåƒ: {self.mirror_url}")
        
        if token:
            env["HF_TOKEN"] = token
            logger.info("ğŸ” ä½¿ç”¨æä¾›çš„token")
        
        try:
            cmd = [
                "huggingface-cli", "download",
                "--repo-type", "dataset",
                "--resume-download",
                repo_name,
                "--local-dir", str(local_path)
            ]
            
            logger.info(f"ğŸš€ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            result = subprocess.run(
                cmd,
                env=env,
                capture_output=True,
                text=True,
                check=True
            )
            
            logger.info("âœ… CLI ä¸‹è½½æˆåŠŸ")
            logger.info(f"ğŸ“ ä¸‹è½½ç›®å½•: {local_path}")
            return str(local_path)
            
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ CLI ä¸‹è½½å¤±è´¥: {e}")
            logger.error(f"stderr: {e.stderr}")
            raise
        except FileNotFoundError:
            logger.error("âŒ æœªæ‰¾åˆ° huggingface-cli å‘½ä»¤")
            logger.info("ğŸ’¡ è¯·å®‰è£…: pip install huggingface_hub[cli]")
            raise
    
    def load_parquet_from_download(self, local_dir: str) -> List[Dict]:
        """
        ä»ä¸‹è½½çš„ç›®å½•ä¸­åŠ è½½parquetæ–‡ä»¶
        
        Args:
            local_dir: ä¸‹è½½çš„æœ¬åœ°ç›®å½•
            
        Returns:
            æ•°æ®åˆ—è¡¨
        """
        local_path = Path(local_dir)
        data_dir = local_path / "data"
        
        if not data_dir.exists():
            data_dir = local_path
        
        parquet_files = list(data_dir.glob("*.parquet"))
        
        if not parquet_files:
            raise FileNotFoundError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ° parquet æ–‡ä»¶")
        
        logger.info(f"ğŸ“„ æ‰¾åˆ° {len(parquet_files)} ä¸ªparquetæ–‡ä»¶")
        
        all_data = []
        for parquet_file in parquet_files:
            logger.info(f"ğŸ“– åŠ è½½æ–‡ä»¶: {parquet_file.name}")
            try:
                df = pd.read_parquet(parquet_file)
                data = self._convert_df_to_json_compatible(df)
                all_data.extend(data)
                logger.info(f"âœ… åŠ è½½ {len(data)} æ¡æ•°æ®")
            except Exception as e:
                logger.error(f"âŒ åŠ è½½ {parquet_file.name} å¤±è´¥: {e}")
                raise
        
        logger.info(f"ğŸ“‹ æ€»è®¡åŠ è½½: {len(all_data)} æ¡æ•°æ®")
        return all_data
    
    def _convert_df_to_json_compatible(self, df: pd.DataFrame) -> List[Dict]:
        """
        å°†DataFrameè½¬æ¢ä¸ºJSONå…¼å®¹çš„æ•°æ®åˆ—è¡¨
        
        Args:
            df: pandas DataFrame
            
        Returns:
            JSONå…¼å®¹çš„å­—å…¸åˆ—è¡¨
        """
        data = []
        for record in df.to_dict('records'):
            clean_record = {}
            for key, value in record.items():
                if value is None:
                    clean_record[key] = None
                elif hasattr(value, 'tolist'):
                    clean_record[key] = value.tolist()
                elif hasattr(value, 'item') and hasattr(value, 'shape') and value.shape == ():
                    clean_record[key] = value.item()
                elif hasattr(value, '__len__') and not isinstance(value, (str, bytes)):
                    try:
                        clean_record[key] = list(value)
                    except Exception:
                        clean_record[key] = str(value)
                else:
                    clean_record[key] = value
            data.append(clean_record)
        return data
    
    def download_dataset(
        self, 
        repo_name: str, 
        split: Optional[str] = None, 
        token: Optional[str] = None,
        use_cli_fallback: bool = True
    ) -> Dict[str, List[Dict]]:
        """
        ä»Hugging Face Hubä¸‹è½½æ•°æ®é›†
        
        Args:
            repo_name: ä»“åº“åç§°
            split: æ•°æ®åˆ†å‰²åç§°ï¼ˆå¦‚ 'train', 'validation', 'test'ï¼‰
            token: è®¿é—®ä»¤ç‰Œ
            use_cli_fallback: å¦‚æœdatasetsåº“å¤±è´¥ï¼Œæ˜¯å¦å°è¯•ä½¿ç”¨CLIå·¥å…·
            
        Returns:
            æŒ‰åˆ†å‰²åç§°ç»„ç»‡çš„æ•°æ®å­—å…¸
        """
        # éªŒè¯token
        if token:
            logger.info("ğŸ” æ­£åœ¨éªŒè¯ Hugging Face token...")
            if not self.validate_token(token):
                raise ValueError("âŒ æä¾›çš„ token æ— æ•ˆ")
            
            logger.info("ğŸ” æ­£åœ¨æ£€æŸ¥æ•°æ®é›†è®¿é—®æƒé™...")
            if not self.check_dataset_access(repo_name, token):
                raise ValueError("âŒ æ²¡æœ‰è®¿é—®è¯¥æ•°æ®é›†çš„æƒé™")
        
        if self.use_mirror:
            logger.info(f"ğŸ“¥ æ­£åœ¨é€šè¿‡é•œåƒ {self.mirror_url} ä¸‹è½½æ•°æ®é›†: {repo_name}")
        else:
            logger.info(f"ğŸ“¥ æ­£åœ¨ä»å®˜æ–¹æºä¸‹è½½æ•°æ®é›†: {repo_name}")
        
        # ç™»å½•
        if token:
            login(token=token)
            logger.info("ğŸ” ä½¿ç”¨æä¾›çš„tokenç™»å½•")
        else:
            env_token = os.getenv('HF_TOKEN')
            if env_token:
                login(token=env_token)
                token = env_token
                logger.info("ğŸ” ä½¿ç”¨ç¯å¢ƒå˜é‡HF_TOKENç™»å½•")
        
        try:
            # ä½¿ç”¨ datasets åº“ä¸‹è½½
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨ datasets åº“ä¸‹è½½...")
            
            if split:
                dataset = load_dataset(repo_name, split=split, token=token)
                logger.info(f"âœ… æˆåŠŸä¸‹è½½ {split} åˆ†å‰²ï¼Œå…± {len(dataset)} æ¡æ•°æ®")
                data = {split: [dict(item) for item in dataset]}
            else:
                dataset = load_dataset(repo_name, token=token)
                logger.info(f"âœ… æˆåŠŸä¸‹è½½æ•°æ®é›†ï¼ŒåŒ…å«åˆ†å‰²: {list(dataset.keys())}")
                
                data = {}
                for split_name, split_data in dataset.items():
                    data[split_name] = [dict(item) for item in split_data]
                    logger.info(f"ğŸ“Š {split_name}: {len(data[split_name])} æ¡æ•°æ®")
            
            return data
            
        except Exception as e:
            logger.error(f"âŒ datasets åº“ä¸‹è½½å¤±è´¥: {str(e)}")
            
            if use_cli_fallback:
                logger.info("\nğŸ”„ å°è¯•ä½¿ç”¨ CLI å·¥å…·ä¸‹è½½...")
                try:
                    temp_dir = f"./temp_{repo_name.split('/')[-1]}"
                    local_dir = self.download_dataset_cli(repo_name, token, temp_dir)
                    all_data = self.load_parquet_from_download(local_dir)
                    
                    # æ¸…ç†ä¸´æ—¶ç›®å½•
                    try:
                        shutil.rmtree(local_dir)
                        logger.info(f"ğŸ§¹ æ¸…ç†ä¸´æ—¶ç›®å½•: {local_dir}")
                    except Exception:
                        logger.warning(f"âš ï¸ æ— æ³•æ¸…ç†ä¸´æ—¶ç›®å½•: {local_dir}")
                    
                    return {"all": all_data}
                    
                except Exception as cli_error:
                    logger.error(f"âŒ CLI ä¸‹è½½ä¹Ÿå¤±è´¥: {str(cli_error)}")
            
            raise Exception(f"æ‰€æœ‰ä¸‹è½½æ–¹æ³•éƒ½å¤±è´¥äº†ã€‚é”™è¯¯: {str(e)}")
    
    def save_to_json(
        self, 
        data: Dict[str, List[Dict]], 
        output_dir: Union[str, Path],
        format_type: str = "list",
        indent: int = 2
    ) -> List[str]:
        """
        å°†æ•°æ®ä¿å­˜ä¸ºJSONæ–‡ä»¶
        
        Args:
            data: æŒ‰åˆ†å‰²åç§°ç»„ç»‡çš„æ•°æ®å­—å…¸
            output_dir: è¾“å‡ºç›®å½•
            format_type: JSONæ ¼å¼ç±»å‹ ('list', 'data_wrapper', 'lines')
            indent: JSONç¼©è¿›ç©ºæ ¼æ•°
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        for split_name, split_data in data.items():
            output_path = output_dir / f"LingxiDiag-16K_{split_name}_data.json"
            
            logger.info(f"ğŸ’¾ æ­£åœ¨ä¿å­˜ {split_name} åˆ°: {output_path}")
            
            if format_type == "list":
                output_data = split_data
            elif format_type == "data_wrapper":
                output_data = {"data": split_data}
            elif format_type == "lines":
                with open(output_path, 'w', encoding='utf-8') as f:
                    for item in split_data:
                        f.write(json.dumps(item, ensure_ascii=False) + '\n')
                logger.info(f"âœ… å·²ä¿å­˜ä¸ºJSONLæ ¼å¼ï¼Œå…± {len(split_data)} æ¡æ•°æ®")
                saved_files.append(str(output_path))
                continue
            else:
                raise ValueError("format_type å¿…é¡»æ˜¯ 'list', 'data_wrapper' æˆ– 'lines'")
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, ensure_ascii=False, indent=indent)
            
            logger.info(f"âœ… å·²ä¿å­˜ä¸ºJSONæ ¼å¼ï¼Œå…± {len(split_data)} æ¡æ•°æ®")
            saved_files.append(str(output_path))
        
        return saved_files
    
    def download_and_save(
        self, 
        repo_name: str, 
        output_dir: Union[str, Path],
        split: Optional[str] = None, 
        token: Optional[str] = None,
        format_type: str = "list",
        indent: int = 2
    ) -> List[str]:
        """
        ä¸‹è½½æ•°æ®é›†å¹¶ä¿å­˜ä¸ºJSONæ–‡ä»¶
        
        Args:
            repo_name: ä»“åº“åç§°
            output_dir: è¾“å‡ºç›®å½•
            split: æ•°æ®åˆ†å‰²åç§°
            token: è®¿é—®ä»¤ç‰Œ
            format_type: JSONæ ¼å¼ç±»å‹
            indent: JSONç¼©è¿›
            
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        # ä¸‹è½½æ•°æ®
        data = self.download_dataset(repo_name, split, token)
        
        # ä¿å­˜ä¸ºJSON
        saved_files = self.save_to_json(data, output_dir, format_type, indent)
        
        return saved_files


def main():
    parser = argparse.ArgumentParser(
        description='ä»Hugging Face Hubä¸‹è½½LingxiDiag-16Kæ•°æ®é›†',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    
    parser.add_argument(
        '--repo-name',
        type=str,
        required=True,
        help='Hugging Faceä»“åº“åç§°ï¼Œæ ¼å¼ä¸º "username/dataset-name"'
    )
    
    parser.add_argument(
        '--output-dir',
        type=str,
        default=str(PROJECT_ROOT / "downloaded_data"),
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: ./downloaded_dataï¼‰'
    )
    
    parser.add_argument(
        '--token',
        type=str,
        default=None,
        help='Hugging Faceè®¿é—®ä»¤ç‰Œï¼ˆä¹Ÿå¯é€šè¿‡HF_TOKENç¯å¢ƒå˜é‡è®¾ç½®ï¼‰'
    )
    
    parser.add_argument(
        '--split',
        type=str,
        default=None,
        choices=['train', 'validation', 'test'],
        help='è¦ä¸‹è½½çš„æ•°æ®åˆ†å‰²ï¼ˆé»˜è®¤ä¸‹è½½å…¨éƒ¨ï¼‰'
    )
    
    parser.add_argument(
        '--format',
        type=str,
        default='list',
        choices=['list', 'data_wrapper', 'lines'],
        help='è¾“å‡ºJSONæ ¼å¼ï¼ˆé»˜è®¤: listï¼‰'
    )
    
    parser.add_argument(
        '--no-mirror',
        action='store_true',
        help='ä¸ä½¿ç”¨hf-mirroré•œåƒï¼ˆé»˜è®¤ä½¿ç”¨é•œåƒåŠ é€Ÿï¼‰'
    )
    
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("å¼€å§‹ä¸‹è½½LingxiDiag-16Kæ•°æ®é›†")
    logger.info("=" * 60)
    
    # åˆ›å»ºä¸‹è½½å™¨
    downloader = LingxiDatasetDownloader(use_mirror=not args.no_mirror)
    
    # ä¸‹è½½å¹¶ä¿å­˜
    saved_files = downloader.download_and_save(
        repo_name=args.repo_name,
        output_dir=args.output_dir,
        split=args.split,
        token=args.token,
        format_type=args.format
    )
    
    logger.info("=" * 60)
    logger.info("âœ… æ•°æ®é›†ä¸‹è½½å®Œæˆï¼")
    for file_path in saved_files:
        logger.info(f"ğŸ“„ ä¿å­˜æ–‡ä»¶: {file_path}")
    logger.info("=" * 60)


if __name__ == '__main__':
    main()

